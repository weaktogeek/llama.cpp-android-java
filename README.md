# llama.cpp-android-java

> **Offline AI Chat for Android â€” Run Llama.cpp locally in Java with no internet, no cloud, full privacy.**

![Android](https://img.shields.io/badge/Platform-Android-brightgreen?logo=android)
![Language](https://img.shields.io/badge/Language-Java-blue?logo=openjdk)
![LLM](https://img.shields.io/badge/Powered_by-llama.cpp-orange?logo=meta)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

---

## ðŸ“– Overview

**ThinkOffline** is an Android app that runs **Llama.cpp models fully on-device**, written in **Java** and integrated through **JNI (Java Native Interface)**.  
It provides an **offline AI chat experience** â€” no internet required, no cloud inference, and complete data privacy.

This project showcases how to embed **Llama.cpp** inside an Android app, load **GGUF models**, and perform **text generation locally**.

---

## âœ¨ Features

âœ… 100% **offline LLM inference** â€” no network calls  
âœ… **Java + JNI** bridge to native Llama.cpp  
âœ… **Local model loader** for GGUF models  
âœ… **Streaming chat interface** built with RecyclerView  
âœ… **Works on Android 8.0+ (API 26+)**  
âœ… Optional **Vulkan / NNAPI acceleration**  
âœ… **Privacy-first design** â€” your data never leaves your phone

---

## ðŸ§© Architecture

ThinkOffline/
â”œâ”€â”€ app/ # Android Java app (UI, ViewModel, JNI bridge)
â”œâ”€â”€ llama.cpp/ # Native C++ inference engine
â”œâ”€â”€ models/ # GGUF model files (e.g., llama-2-7b.Q4_K_M.gguf)
â””â”€â”€ README.md

## ðŸ§  Future Roadmap

 Add multi-threaded inference settings
 Add token streaming UI
 Implement voice input + TTS reply
 Support model quantization selector
 Optional Vulkan acceleration toggle
